import evaluate
import argparse
import pickle
import numpy as np
def bleurt_score(predictions, references):
    """Compute the average BLEURT score over the gpt responses

    Args:
        predictions (list): List of gpt responses generated by GPT-3.5 under a certain instruction
        references (list): List of sentence 2 collected from raw dataset (e.g. wnli)

    Returns:
        bluert_scores: List of bluert scores
    """
    bluert_scores = bleurt.compute(predictions=predictions,
                                   references=references)['scores']
    return bluert_scores


def rougeL_score(predictions, references):
    """Compute the rougel score over the gpt responses

    Args:
        predictions (list): List of gpt responses generated by GPT-3.5 under a certain instruction
        references (list): List of sentence 2 collected from raw dataset (e.g. wnli)

    Returns:
        rougeL_scores: List of rougeL scores
    """
    rougeL_score = rouge.compute(predictions=predictions,
                                 references=references, use_aggregator=False)['rougeL']
    return rougeL_score

parser = argparse.ArgumentParser()
parser.add_argument("--batch_size", type=int, default=1)
parser.add_argument("--model_size", type=str, default="410m")
parser.add_argument("--dataset_name", type=str, default="all", choices=["arxiv", "dm_mathematics", "github", "hackernews", "pile_cc",
                     "pubmed_central", "wikipedia_(en)", "full_pile","WikiMIA64", "WikiMIA128","WikiMIA256",
                      "WikiMIAall"])
parser.add_argument("--cuda", type=int, default=1, help="cuda device")
parser.add_argument("--skip_calculation", type=str, default="True")
parser.add_argument("--reference_model", type=str, default="True")
parser.add_argument("--samples", type=int, default=1000)
parser.add_argument("--generation_samples", type=int, default=10)
args = parser.parse_args()



bleurt =  evaluate.load('bleurt', 'bleurt-20',
                        model_type="metric")
rouge = evaluate.load('rouge')
dataset_name = "arxiv"
temp_input_length = 48
f = open(f"sem_mem_score_online/{args.model_size}/{dataset_name}_{temp_input_length}_generation_samples.pkl", "rb")
data = pickle.load(f)
f.close()
half = len(data)//2
member_bleurt = []
member_rouge = []
nonmember_bleurt = []
nonmember_rouge = []
for idx, example in enumerate(data):
    original_text = data[0]
    full_generated_texts = data[1]
    partial_generated_texts = data[2]
    references = data[3]
    bleurt_score = np.array(
        bleurt_score(partial_generated_texts, [references for _ in range(args.generation_samples)])).mean()
    rougle_score = np.array(
        rougeL_score(partial_generated_texts, [references for _ in range(args.generation_samples)])).mean()
    if idx < half:
        member_bleurt.append(bleurt_score)
        member_rouge.append(rougle_score)
    else:
        nonmember_bleurt.append(bleurt_score)
        nonmember_rouge.append(rougle_score)



