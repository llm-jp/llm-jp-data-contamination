import os
import random
import argparse
import dotenv
from openai import OpenAI
import evaluate
from utils import *
import pdb
from tqdm import tqdm
import numpy as np
dotenv.load_dotenv()
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
def bleurt_score(predictions, references):
    """Compute the average BLEURT score over the gpt responses
    
    Args: 
        predictions (list): List of gpt responses generated by GPT-3.5 under a certain instruction
        references (list): List of sentence 2 collected from raw dataset (e.g. wnli)
    
    Returns: 
        bluert_scores: List of bluert scores  
    """
    bluert_scores = bleurt.compute(predictions=predictions, 
                            references=references)['scores']
    return bluert_scores

def rougeL_score(predictions, references):
    """Compute the rougel score over the gpt responses
    
    Args: 
        predictions (list): List of gpt responses generated by GPT-3.5 under a certain instruction
        references (list): List of sentence 2 collected from raw dataset (e.g. wnli)
    
    Returns: 
        rougeL_scores: List of rougeL scores  
    """
    rougeL_score = rouge.compute(predictions=predictions, 
                           references=references, use_aggregator=False)['rougeL']
    return rougeL_score

def significance_test(guided_scores, general_scores):
    """Perform significance test to determine whether GPT-3.5 is contaminated

    Args:
        guided_scores (list): List of bleurt scores generated by guided_instruction
        general_scores (list): List of bleurt scores generated by general_instruction

    Returns:
        is_contaminated: True if GPT-3.5 is contaminated, False otherwise
    """
    n_bootstrap = 10000

    # Function to perform bootstrap resampling
    def bootstrap_resample(data, n_samples):
        indices = np.random.randint(0, len(data), (n_samples, len(data)))
        samples = np.array(data)[indices]
        return samples

    # Generate bootstrap samples
    guided_bootstrap = bootstrap_resample(guided_scores, n_bootstrap)
    general_bootstrap = bootstrap_resample(general_scores, n_bootstrap)

    # Calculate the means for each bootstrap sample
    guided_means = np.mean(guided_bootstrap, axis=1)
    general_means = np.mean(general_bootstrap, axis=1)

    # Calculate the differences in means
    mean_diffs = guided_means - general_means

    # Observed difference in the original dataset
    observed_diff = np.mean(guided_scores) - np.mean(general_scores)

    # Calculate the p-value
    p_value = np.mean(mean_diffs >= observed_diff)

    print(f"Observed difference: {observed_diff}")
    print(f"p-value: {p_value}")

    # Check for statistical significance
    if p_value <= 0.05:
        print("The difference is statistically significant.")
        return True
    else:
        print("The difference is not statistically significant.")
        return False

def is_contaminated(dataset, task_name, dataset_name):
    """Confirm wether GPT-3.5 is contaminated on a given dataset
    
    Args: 
        dataset (jsonl): GPT-3.5 responses under guided_instructions and general_instructions
        task_name (str): name of task (e.g. nli-task)
        dataset_name (str): name of dataset (e.g. wnli)
    
    Returns: 
        bluert_scores: List of bluert scores  
        
    TO-DO:
    If the gap between two bleurt scores is significant and total #samples satisfy the predicate: contaminated, 
    else suspicious (0 < #instances < threshold), else clean(#instance satisfy the predicate = 0)             
    """
    guided_responses = []
    general_responses = []
    references = []
    for idx in range(len(dataset)):
        guided_responses += [dataset[idx]['guided_instruction']['candidate']]
        general_responses += [dataset[idx]['general_instruction']['candidate']]
        references += [dataset[idx]['guided_instruction']['reference']]
    
    print("......Starting compute bleurt score and rougeL score......")
    
    bleurt_scores = (bleurt_score(guided_responses, references),
                     bleurt_score(general_responses, references))
    rougeL_scores = (rougeL_score(guided_responses, references),
                     rougeL_score(general_responses, references))
    bleurt_indicator = significance_test(bleurt_scores[0], bleurt_scores[1])
    rougel_indicator = significance_test(rougeL_scores[0], rougeL_scores[1])
    print('......Eval Results......\n', bleurt_scores, '\n', rougeL_scores, '\n')
    
    save_jsonl({
        "average bleurt score":  [sum(x)/len(references) for x in bleurt_scores],
        "rougeL score": [sum(x)/len(references) for x in rougeL_scores],
        "bleurt_indicator": bleurt_indicator,
        "rougel_indicator": rougel_indicator
    }, f'out/{dataset_name}/train/data_contamination_result.jsonl')
    
    print("......Successfully saved eval result......")
    

def create_random_samples(dataset, num_samples=15):
    """Create random samples for contamination detection test
    
    Args: 
        dataset (jsonl): raw data include two types of instructions: guided_instruction, general_instruction
        num_samples (int): number of samples (e.g. 15 or 20)
      
    Returns: 
        samples: randomly selected samples  
    """
    return random.sample(dataset, num_samples)

def get_gpt_responses(instruction, sentence1, 
                      label, 
                      model="gpt-3.5-turbo", 
                      max_tokens=500, 
                      temperature=0):
    """Obtain gpt responses for contamination detection test
    
    Args: 
        instruction (str): the content of guided_instruction or general_instruction
        sentence1 (str): as partial prompt
        label (int): as partial prompt
        model (str): GPT model's name
        max_tokens (int): maxmimum tokens of generated gpt response
        temperature (int): parameter
      
    Returns: 
        samples: gpt response  
    """
    # return client.chat.completions.create(
    #         model="gpt-3.5-turbo",
    #         max_tokens=500,
    #         temperature=0,
    #         messages=[
    #             {"role": "system", "content": instruction},
    #             {"role": "user", "content": f"sentence1: {sentence1}\n\nlabel: {label}\n"}
    #         ]
    #     ).choices[0].message.content
    pass

def save_gpt_responses(random_samples, task_name, 
                       dataset_name, 
                       split_name,
                       model, 
                       max_tokens, 
                       temperature):
    """Create dataset for contamination detection test
    
    Args: 
        random_samples (jsonl): randomly selected samples
        task_name (str): e.g. nli-task
        dataset_name (str): e.g. wnli
        model (str): GPT model's name
        max_tokens (int): maxmimum tokens of generated gpt response
        temperature (int): parameter
      
    """
    new_instructions = []
    num_instructions = len(random_samples)
    instrc_type_instrc = load_json(f"data/{task_name}/{dataset_name}/instructions.json")
    print("......Starting to get gpt responses......")
    for idx in range(num_instructions):
        new_instruction = {}
        for inst_type in ['guided_instruction', 'general_instruction']:
            instruction = random_samples[idx][inst_type]
            response = get_gpt_responses(instruction['instruction'], 
                                         instruction['sentence1'], 
                                         instruction['label'],
                                         model=model,
                                         max_tokens=max_tokens,
                                         temperature=temperature)

            new_instruction.update({
                    inst_type: {
                    "instruction": instrc_type_instrc[task_name][inst_type],  
                    "sentence1": instruction['sentence1'], 
                    "candidate": response,
                    "reference": instruction['sentence2'],
                    "label": instruction['label']
                    }
                })
            
        new_instructions.append(new_instruction)
    save_jsonl(new_instructions, f'data/{task_name}/{dataset_name}/{model}_response_{split_name}.jsonl')
            
    print(".......Successfully saved generated gpt reponses......")

def get_llmjp_response(random_samples, task_name,
                       dataset_name,
                       split_name,
                       model,
                       max_tokens,
                       temperature):
    tokenizer = AutoTokenizer.from_pretrained(
        "llm-jp/llm-jp-13b-instruct-full-dolly-ichikara_004_001_single-oasst-oasst2-v2.0")
    model = AutoModelForCausalLM.from_pretrained(
        "llm-jp/llm-jp-13b-instruct-full-dolly-ichikara_004_001_single-oasst-oasst2-v2.0", device_map="auto",
        torch_dtype=torch.bfloat16)
    new_instructions = []
    guided_chat = [
        {"role": "system", "content": f"### 指示：次の文1は、{dataset_name} データセットの{split_name}分割から提供されています。データセットに表示された通りに、文2を完成させてください。\nデータセットに表示された通りに、文 2を完成させてください。\n文2はデータセット内のインスタンスと完全に一致する必要があります。"},
        {"role": "user", "content": ""},
    ]
    general_chat = [
        {"role": "system", "content": "### 指示：文1に基づいて文2を完成させてください。次のラベルは、文1と文2の間の論理的な関係を示します。"},
        {"role": "user", "content": ""},
    ]
    guided_chat_template = "{% for message in messages %}{% if message['role'] == 'user' %}{{ '\\n\\n### 指示:\\n' + message['content'] }}{% elif message['role'] == 'system' %}{{ '### 指示：jnliデータセットのtrain分割から文1が提供される。データセットに現れた文2を完成させなさい。文2はデータセットのサンプルと正確に一致しなければならないです。' }}{% elif message['role'] == 'assistant' %}{{ '\\n\\n### 応答:\\n' + message['content'] + eos_token }}{% endif %}{% if loop.last and add_generation_prompt %}{{ '\\n\\n### 応答:\\n' }}{% endif %}{% endfor %}"
    general_chat_template = "{% for message in messages %}{% if message['role'] == 'user' %}{{ '\\n\\n### 指示:\\n' + message['content'] }}{% elif message['role'] == 'system' %}{{ '### 指示：以下のラベルが文1と文2の論理的関係を示すように、文1を基に文2を完成させる。' }}{% elif message['role'] == 'assistant' %}{{ '\\n\\n### 応答:\\n' + message['content'] + eos_token }}{% endif %}{% if loop.last and add_generation_prompt %}{{ '\\n\\n### 応答:\\n' }}{% endif %}{% endfor %}"
    for idx in tqdm(range(len(random_samples))):
        new_instruction = {}
        for inst_type in ['guided_instruction', 'general_instruction']:
            instruction = guided_chat[0]["content"] if inst_type == 'guided_instruction' else general_chat[0]["content"]
            example = random_samples[idx]
            sent1=example['input'].split("\n")[0]
            sent2=example['input'].split("\n")[1]
            label = "含意" if example['output'] == "entailment" else "矛盾" if example['output'] == "contradiction" else "中立"
            if inst_type == 'guided_instruction':
                chat = guided_chat
                chat[1]["content"] = f"{sent1}\nラベル:{label}\n"
                tokenized_input = tokenizer.apply_chat_template(chat, guided_chat_template, add_generation_prompt=True,
                                                                tokenize=True,
                                                                return_tensors="pt").to(model.device)
            else:
                chat = general_chat
                chat[1]["content"] = f"{sent1}\nラベル:{label}\n"
                tokenized_input = tokenizer.apply_chat_template(chat, general_chat_template, add_generation_prompt=True,
                                                                tokenize=True,
                                                                return_tensors="pt").to(model.device)
            with torch.no_grad():
                output = model.generate(
                    tokenized_input,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    top_p=0.95,
                    temperature=0.0001,
                    repetition_penalty=1.05,
                )[0]
            input_length = tokenized_input.size()[1]
            generated_text_tokens = output[input_length:]
            response = tokenizer.decode(generated_text_tokens.tolist()).replace("<EOD|LLM-jp>", "")
            new_instruction.update({
                inst_type: {
                    "instruction": instruction,
                    "sentence1": sent1,
                    "candidate": response,
                    "reference": sent2,
                    "label": example['output']
                }
            })
        new_instructions.append(new_instruction)
    save_jsonl(new_instructions, f'data/{task_name}/{dataset_name}/{split_name}/llmjp_response.jsonl')
    print(".......Successfully saved generated gpt reponses......")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset_name",
                        type=str,
                        default="jnli",
                        help="the name of dataset")
    parser.add_argument("--task_name",
                        type=str,
                        default="nli-task",
                        help="the category of task")
    parser.add_argument("--split_name",
                        type=str,
                        default="train",
                        help="the partition of dataset")
    parser.add_argument("--mode",
                        type=str,
                        default="llm-jp",
                        help="generate gpt responses or eval gpt responses by metrics")
    parser.add_argument("--data_path",
                        type=str,
                        default="data/{task_name}/{dataset_name}/{split_name}.jsonl",
                        help="the path of dataset")
    parser.add_argument("--model",
                        type=str,
                        default="llmjp",
                        help="the name of model")
    args = parser.parse_args()

    bleurt =  evaluate.load('bleurt', 'bleurt-20', model_type="metric")
    rouge = evaluate.load('rouge')
    print(args)
    if args.mode == "eval":
        #eval gpt responses by metrics
        responses = load_json(f'data/{args.task_name}/{args.dataset_name}/{args.split_name}/{args.model}_response.jsonl')
        is_contaminated(responses, args.task_name, args.dataset_name)
    elif args.model == "OpenAI":
        #create gpt responses for LMs contamination detection test
        wnli_train = load_json(f"data/{args.task_name}/{args.dataset_name}/{args.split_name}.jsonl")
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        random_samples = create_random_samples(wnli_train, num_samples=15)
        save_gpt_responses(random_samples,
                           task_name=args.task_name,
                           dataset_name=args.dataset_name,
                           split_name=args.split_name,
                           model=args.model,
                           max_tokens=500,
                           temperature=0)
    elif args.mode == "llm-jp":
        print("evaluation for llm-jp model...")
        loaded_data = load_json(f"datasets_contamination/1.3.0/evaluation/{args.split_name}/{args.dataset_name}.json")
        random_samples = create_random_samples(loaded_data["samples"], num_samples=100)
        get_llmjp_response(random_samples,
                           task_name=args.task_name,
                           dataset_name=args.dataset_name,
                           split_name=args.split_name,
                           model=args.model,
                           max_tokens=500,
                           temperature=0)
    