import os
import random
import argparse
import dotenv
from openai import OpenAI
import evaluate
from utils import *

dotenv.load_dotenv()

def bleurt_score(predictions, references):
    """Compute the average BLEURT score over the gpt responses
    
    Args: 
        predictions (list): List of gpt responses generated by GPT-3.5 under a certain instruction
        references (list): List of sentence 2 collected from raw dataset (e.g. wnli)
    
    Returns: 
        bluert_scores: List of bluert scores  
    """
    bluert_scores = bleurt.compute(predictions=predictions, 
                            references=references)['scores']
    return bluert_scores

def rougeL_score(predictions, references):
    """Compute the rougel score over the gpt responses
    
    Args: 
        predictions (list): List of gpt responses generated by GPT-3.5 under a certain instruction
        references (list): List of sentence 2 collected from raw dataset (e.g. wnli)
    
    Returns: 
        rougeL_scores: List of rougeL scores  
    """
    rougeL_score = rouge.compute(predictions=predictions, 
                           references=references)['rougeL']
    return rougeL_score

def is_contaminated(dataset, task_name, dataset_name):
    """Confirm wether GPT-3.5 is contaminated on a given dataset
    
    Args: 
        dataset (jsonl): GPT-3.5 responses under guided_instructions and general_instructions
        task_name (str): name of task (e.g. nli-task)
        dataset_name (str): name of dataset (e.g. wnli)
    
    Returns: 
        bluert_scores: List of bluert scores  
        
    TO-DO:
    If the gap between two bleurt scores is significant and total #samples satisfy the predicate: contaminated, 
    else suspicious (0 < #instances < threshold), else clean(#instance satisfy the predicate = 0)             
    """
    guided_responses = []
    general_responses = []
    references = []
    for idx in range(len(dataset)):
        guided_responses += [dataset[idx]['guided_instruction']['candidate']]
        general_responses += [dataset[idx]['general_instruction']['candidate']]
        references += [dataset[idx]['guided_instruction']['reference']]
    
    print("......Starting compute bleurt score and rougeL score......")
    
    bleurt_scores = (sum(bleurt_score(guided_responses, references)) / len(references), 
                     sum(bleurt_score(general_responses, references)) / len(references))
    rougeL_scores = (rougeL_score(guided_responses, references), rougeL_score(general_responses, references))
    
    print('......Eval Results......\n', bleurt_scores, '\n', rougeL_scores, '\n')
    
    save_jsonl({
        "average bleurt score": bleurt_scores,
        "rougeL score": rougeL_scores
    }, f'out/{task_name}/{dataset_name}/eval.jsonl')
    
    print("......Successfully saved eval result......")
    

def create_random_samples(dataset, num_samples=15):
    """Create random samples for contamination detection test
    
    Args: 
        dataset (jsonl): raw data include two types of instructions: guided_instruction, general_instruction
        num_samples (int): number of samples (e.g. 15 or 20)
      
    Returns: 
        samples: randomly selected samples  
    """
    return random.sample(dataset, num_samples)

def get_gpt_responses(instruction, sentence1, 
                      label, 
                      model="gpt-3.5-turbo", 
                      max_tokens=500, 
                      temperature=0):
    """Obtain gpt responses for contamination detection test
    
    Args: 
        instruction (str): the content of guided_instruction or general_instruction
        sentence1 (str): as partial prompt
        label (int): as partial prompt
        model (str): GPT model's name
        max_tokens (int): maxmimum tokens of generated gpt response
        temperature (int): parameter
      
    Returns: 
        samples: gpt response  
    """
    return client.chat.completions.create(
            model="gpt-3.5-turbo",
            max_tokens=500,
            temperature=0,
            messages=[
                {"role": "system", "content": instruction},
                {"role": "user", "content": f"sentence1: {sentence1}\n\nlabel: {label}\n"}
            ]
        ).choices[0].message.content

def save_gpt_responses(random_samples, task_name, 
                       dataset_name, 
                       split_name,
                       model, 
                       max_tokens, 
                       temperature):
    """Create dataset for contamination detection test
    
    Args: 
        random_samples (jsonl): randomly selected samples
        task_name (str): e.g. nli-task
        dataset_name (str): e.g. wnli
        model (str): GPT model's name
        max_tokens (int): maxmimum tokens of generated gpt response
        temperature (int): parameter
      
    """
    new_instructions = []
    num_instructions = len(random_samples)
    instrc_type_instrc = load_json(f"data/{task_name}/{dataset_name}/instructions.json")
    print("......Starting to get gpt responses......")
    for idx in range(num_instructions):
        new_instruction = {}
        for inst_type in ['guided_instruction', 'general_instruction']:
            instruction = random_samples[idx][inst_type]
            response = get_gpt_responses(instruction['instruction'], 
                                         instruction['sentence1'], 
                                         instruction['label'],
                                         model=model,
                                         max_tokens=max_tokens,
                                         temperature=temperature)

            new_instruction.update({
                    inst_type: {
                    "instruction": instrc_type_instrc[task_name][inst_type],  
                    "sentence1": instruction['sentence1'], 
                    "candidate": response,
                    "reference": instruction['sentence2'],
                    "label": instruction['label']
                    }
                })
            
        new_instructions.append(new_instruction)
    save_jsonl(new_instructions, f'data/{task_name}/{dataset_name}/gpt_response_{split_name}.jsonl')
            
    print(".......Successfully saved generated gpt reponses......")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset_name", 
                        type=str,
                        default="wnli",
                        help="the name of dataset")
    parser.add_argument("--task_name", 
                        type=str,
                        default="nli-task",
                        help="the category of task")
    parser.add_argument("--split_name", 
                        type=str,
                        default="train",
                        help="the partition of dataset")
    parser.add_argument("--mode", 
                        type=str,
                        default="get",
                        help="generate gpt responses or eval gpt responses by metrics")
    parser.add_argument("--data_path", 
                        type=str,
                        default="data/{task_name}/{dataset_name}/{split_name}.jsonl",
                        help="the path of dataset")
    args = parser.parse_args()
    
    bleurt =  evaluate.load('bleurt', 'bleurt-20', model_type="metric")
    rouge = evaluate.load('rouge')
    
    if args.mode == "eval":
        #eval gpt responses by metrics
        gpt_responses = load_json(f'data/{args.task_name}/{args.dataset_name}/gpt_response_{args.split_name}.jsonl')
        is_contaminated(gpt_responses, args.task_name, args.dataset_name)
    else:
        #create gpt responses for LMs contamination detection test
        wnli_train = load_json(args.data_path)
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        random_samples = create_random_samples(wnli_train, num_samples=15)
        save_gpt_responses(random_samples, 
                           task_name=args.task_name,
                           dataset_name=args.dataset_name, 
                           split_name=args.split_name,
                           model="gpt-3.5-turbo",
                           max_tokens=500,
                           temperature=0)
    